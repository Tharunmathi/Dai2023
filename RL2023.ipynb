{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvNjwG8PJ10U"
      },
      "outputs": [],
      "source": [
        "# MARKOV DECISION PROCESS USING DYNAMIC PROGRAMMING\n",
        "trans_data = pd.read_csv(r'transitions_data.csv',header=None)\n",
        "rwd_data = pd.read_csv('rewards.csv',header=None)\n",
        "trans_data = trans_data.to_numpy()\n",
        "rwd_data = rwd_data.to_numpy()\n",
        "\n",
        "gamma = 0.9\n",
        "transitions = {}\n",
        "len_data = trans_data.shape[0]\n",
        "td = trans_data\n",
        "for i in range(1,len_data):   \n",
        "  if (td[i][0] in transitions):\n",
        "    if td[i][1] in transitions[td[i][0]]:\n",
        "      transitions[td[i][0]][td[i][1]].append((float(td[i][3]),td[i][2]))\n",
        "    else:\n",
        "      transitions[td[i][0]][td[i][1]] = [(float(td[i][3]),td[i][2])]\n",
        "  else:\n",
        "    transitions[td[i][0]] = {td[i][1]:[(float(td[i][3]),td[i][2])]}\n",
        "\n",
        "\n",
        "rewards = {}\n",
        "rd = rwd_data\n",
        "len_rewards = rd.shape[0]\n",
        "for i in range(0,len_rewards):\n",
        "  rewards[rd[i][0]] = float(rd[i][1]) if rd[i][1] != 'None' else np.nan\n",
        "\n",
        "class MarkovDecisionProcess:\n",
        "  def __init__(self, states=[], transition={}, reward={}, gamma=0.9):\n",
        "    self.states = states\n",
        "    self.transition = transition\n",
        "    self.reward = reward\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def Rwd(self, state):\n",
        "    return self.reward[state]\n",
        "  \n",
        "  def Trans(self, state, action):\n",
        "    return self.transition[state][action]\n",
        "\n",
        "  def action(self, state):\n",
        "    return self.transition[state].keys()\n",
        "\n",
        "Transitions = transitions\n",
        "Rewards = rewards\n",
        "States = transitions.keys()\n",
        "\n",
        "mdp = MarkovDecisionProcess(states = States, transition = Transitions, reward = Rewards)\n",
        "epsilon = 0.2\n",
        "def val_iteration():\n",
        "  states = mdp.states\n",
        "  actions = mdp.action\n",
        "  Trans = mdp.Trans\n",
        "  Rwd = mdp.Rwd\n",
        "  V1 = {s: 0 for s in states}\n",
        "  while True:\n",
        "    V = V1.copy()\n",
        "    delta = 0 \n",
        "\n",
        "    for s in states:\n",
        "      V1[s] = Rwd(s) + gamma * max([sum([p*V[s1] for (p,s1) in Trans(s,a)]) for a in actions(s)])\n",
        "      delta = max(delta, abs(V1[s]-V[s]))\n",
        "\n",
        "    if (delta < epsilon*(1-gamma)/gamma):\n",
        "      return V\n",
        "\n",
        "def best_policy(V):\n",
        "  states = mdp.states\n",
        "  actions = mdp.action\n",
        "  pi_policy = {}\n",
        "  for s in states:\n",
        "    pi_policy[s] = max(actions(s), key=lambda a: expected_utility(a,s,V))\n",
        "  return pi_policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# monte carlo \n",
        "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
        "    if not policy:\n",
        "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
        "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
        "    returns = {} # 3.\n",
        "    \n",
        "    for _ in range(episodes): # Looping through episodes\n",
        "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
        "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
        "        \n",
        "        # for loop through reversed indices of episode array. \n",
        "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
        "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
        "        \n",
        "        for i in reversed(range(0, len(episode))):   \n",
        "            s_t, a_t, r_t = episode[i] \n",
        "            state_action = (s_t, a_t)\n",
        "            G += r_t # Increment total reward by reward on current timestep\n",
        "            \n",
        "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
        "                if returns.get(state_action):\n",
        "                    returns[state_action].append(G)\n",
        "                else:\n",
        "                    returns[state_action] = [G]   \n",
        "                    \n",
        "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
        "                \n",
        "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
        "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
        "                max_Q = random.choice(indices)\n",
        "                \n",
        "                A_star = max_Q # 14.\n",
        "                \n",
        "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
        "                    if a[0] == A_star:\n",
        "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
        "                    else:\n",
        "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
        "\n",
        "    return policy\n",
        "    \n",
        "env = gym.make(\"FrozenLake8x8-v1\")\n",
        "policy = monte_carlo_e_soft(env,episodes = 5000)\n",
        "test_policy(policy,env)"
      ],
      "metadata": {
        "id": "aiQdb9BQLG6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q Learning\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        s = env.reset()\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            a_ = np.argmax(Q[s_, :])\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            env.render()\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, done, info = env.step(a)\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"Reached goal!\")\n",
        "                else:\n",
        "                    print(\"Shit! dead x_x\")\n",
        "                time.sleep(3)\n",
        "                break\n",
        "\n"
      ],
      "metadata": {
        "id": "cdBimPXoLpmy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QvNjwG8PJ10U"
      },
      "outputs": [],
      "source": [
        "# MARKOV DECISION PROCESS USING DYNAMIC PROGRAMMING\n",
        "trans_data = pd.read_csv(r'transitions_data.csv',header=None)\n",
        "rwd_data = pd.read_csv('rewards.csv',header=None)\n",
        "trans_data = trans_data.to_numpy()\n",
        "rwd_data = rwd_data.to_numpy()\n",
        "\n",
        "gamma = 0.9\n",
        "transitions = {}\n",
        "len_data = trans_data.shape[0]\n",
        "td = trans_data\n",
        "for i in range(1,len_data):   \n",
        "  if (td[i][0] in transitions):\n",
        "    if td[i][1] in transitions[td[i][0]]:\n",
        "      transitions[td[i][0]][td[i][1]].append((float(td[i][3]),td[i][2]))\n",
        "    else:\n",
        "      transitions[td[i][0]][td[i][1]] = [(float(td[i][3]),td[i][2])]\n",
        "  else:\n",
        "    transitions[td[i][0]] = {td[i][1]:[(float(td[i][3]),td[i][2])]}\n",
        "\n",
        "\n",
        "rewards = {}\n",
        "rd = rwd_data\n",
        "len_rewards = rd.shape[0]\n",
        "for i in range(0,len_rewards):\n",
        "  rewards[rd[i][0]] = float(rd[i][1]) if rd[i][1] != 'None' else np.nan\n",
        "\n",
        "class MarkovDecisionProcess:\n",
        "  def __init__(self, states=[], transition={}, reward={}, gamma=0.9):\n",
        "    self.states = states\n",
        "    self.transition = transition\n",
        "    self.reward = reward\n",
        "    self.gamma = gamma\n",
        "\n",
        "  def Rwd(self, state):\n",
        "    return self.reward[state]\n",
        "  \n",
        "  def Trans(self, state, action):\n",
        "    return self.transition[state][action]\n",
        "\n",
        "  def action(self, state):\n",
        "    return self.transition[state].keys()\n",
        "\n",
        "Transitions = transitions\n",
        "Rewards = rewards\n",
        "States = transitions.keys()\n",
        "\n",
        "mdp = MarkovDecisionProcess(states = States, transition = Transitions, reward = Rewards)\n",
        "epsilon = 0.2\n",
        "def val_iteration():\n",
        "  states = mdp.states\n",
        "  actions = mdp.action\n",
        "  Trans = mdp.Trans\n",
        "  Rwd = mdp.Rwd\n",
        "  V1 = {s: 0 for s in states}\n",
        "  while True:\n",
        "    V = V1.copy()\n",
        "    delta = 0 \n",
        "\n",
        "    for s in states:\n",
        "      V1[s] = Rwd(s) + gamma * max([sum([p*V[s1] for (p,s1) in Trans(s,a)]) for a in actions(s)])\n",
        "      delta = max(delta, abs(V1[s]-V[s]))\n",
        "\n",
        "    if (delta < epsilon*(1-gamma)/gamma):\n",
        "      return V\n",
        "\n",
        "def best_policy(V):\n",
        "  states = mdp.states\n",
        "  actions = mdp.action\n",
        "  pi_policy = {}\n",
        "  for s in states:\n",
        "    pi_policy[s] = max(actions(s), key=lambda a: expected_utility(a,s,V))\n",
        "  return pi_policy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# monte carlo \n",
        "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
        "    if not policy:\n",
        "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
        "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
        "    returns = {} # 3.\n",
        "    \n",
        "    for _ in range(episodes): # Looping through episodes\n",
        "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
        "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
        "        \n",
        "        # for loop through reversed indices of episode array. \n",
        "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
        "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
        "        \n",
        "        for i in reversed(range(0, len(episode))):   \n",
        "            s_t, a_t, r_t = episode[i] \n",
        "            state_action = (s_t, a_t)\n",
        "            G += r_t # Increment total reward by reward on current timestep\n",
        "            \n",
        "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
        "                if returns.get(state_action):\n",
        "                    returns[state_action].append(G)\n",
        "                else:\n",
        "                    returns[state_action] = [G]   \n",
        "                    \n",
        "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
        "                \n",
        "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
        "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
        "                max_Q = random.choice(indices)\n",
        "                \n",
        "                A_star = max_Q # 14.\n",
        "                \n",
        "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
        "                    if a[0] == A_star:\n",
        "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
        "                    else:\n",
        "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
        "\n",
        "    return policy\n",
        "    \n",
        "env = gym.make(\"FrozenLake8x8-v1\")\n",
        "policy = monte_carlo_e_soft(env,episodes = 5000)\n",
        "test_policy(policy,env)"
      ],
      "metadata": {
        "id": "aiQdb9BQLG6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Q Learning\n",
        "def init_q(s, a, type=\"ones\"):\n",
        "    \"\"\"\n",
        "    @param s the number of states\n",
        "    @param a the number of actions\n",
        "    @param type random, ones or zeros for the initialization\n",
        "    \"\"\"\n",
        "    if type == \"ones\":\n",
        "        return np.ones((s, a))\n",
        "    elif type == \"random\":\n",
        "        return np.random.random((s, a))\n",
        "    elif type == \"zeros\":\n",
        "        return np.zeros((s, a))\n",
        "\n",
        "def qlearning(alpha, gamma, epsilon, episodes, max_steps, n_tests, render = False, test=False):\n",
        "    \"\"\"\n",
        "    @param alpha learning rate\n",
        "    @param gamma decay factor\n",
        "    @param epsilon for exploration\n",
        "    @param max_steps for max step in each episode\n",
        "    @param n_tests number of test episodes\n",
        "    \"\"\"\n",
        "    env = gym.make('Taxi-v3')\n",
        "    n_states, n_actions = env.observation_space.n, env.action_space.n\n",
        "    Q = init_q(n_states, n_actions, type=\"ones\")\n",
        "    timestep_reward = []\n",
        "    for episode in range(episodes):\n",
        "        print(f\"Episode: {episode}\")\n",
        "        s = env.reset()\n",
        "        a = epsilon_greedy(Q, epsilon, n_actions, s)\n",
        "        t = 0\n",
        "        total_reward = 0\n",
        "        done = False\n",
        "        while t < max_steps:\n",
        "            if render:\n",
        "                env.render()\n",
        "            t += 1\n",
        "            s_, reward, done, info = env.step(a)\n",
        "            total_reward += reward\n",
        "            a_ = np.argmax(Q[s_, :])\n",
        "            if done:\n",
        "                Q[s, a] += alpha * ( reward  - Q[s, a] )\n",
        "            else:\n",
        "                Q[s, a] += alpha * ( reward + (gamma * Q[s_, a_]) - Q[s, a] )\n",
        "            s, a = s_, a_\n",
        "            if done:\n",
        "                if render:\n",
        "                    print(f\"This episode took {t} timesteps and reward: {total_reward}\")\n",
        "                timestep_reward.append(total_reward)\n",
        "                break\n",
        "    if render:\n",
        "        print(f\"Here are the Q values:\\n{Q}\\nTesting now:\")\n",
        "    if test:\n",
        "        test_agent(Q, env, n_tests, n_actions)\n",
        "    return timestep_reward\n",
        "\n",
        "\n",
        "def test_agent(Q, env, n_tests, n_actions, delay=1):\n",
        "    for test in range(n_tests):\n",
        "        print(f\"Test #{test}\")\n",
        "        s = env.reset()\n",
        "        done = False\n",
        "        epsilon = 0\n",
        "        while True:\n",
        "            time.sleep(delay)\n",
        "            env.render()\n",
        "            a = epsilon_greedy(Q, epsilon, n_actions, s, train=True)\n",
        "            print(f\"Chose action {a} for state {s}\")\n",
        "            s, reward, done, info = env.step(a)\n",
        "            if done:\n",
        "                if reward > 0:\n",
        "                    print(\"Reached goal!\")\n",
        "                else:\n",
        "                    print(\"Shit! dead x_x\")\n",
        "                time.sleep(3)\n",
        "                break\n",
        "\n"
      ],
      "metadata": {
        "id": "cdBimPXoLpmy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#policy evaluation\n",
        "\n",
        "class MDP:\n",
        "    def __init__(self, transition_probs, rewards, initial_state=None, seed=None):\n",
        "        \"\"\"\n",
        "        Defines an MDP. Compatible with gym Env.\n",
        "        :param transition_probs: transition_probs[s][a][s_next] = P(s_next | s, a)\n",
        "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> prob]\n",
        "            For each state and action, probabilities of next states should sum to 1\n",
        "            If a state has no actions available, it is considered terminal\n",
        "        :param rewards: rewards[s][a][s_next] = r(s,a,s')\n",
        "            A dict[state -> dict] of dicts[action -> dict] of dicts[next_state -> reward]\n",
        "            The reward for anything not mentioned here is zero.\n",
        "        :param get_initial_state: a state where agent starts or a callable() -> state\n",
        "            By default, picks initial state at random.\n",
        "        States and actions can be anything you can use as dict keys, but it is recommended to use strings or integers\n",
        "        \"\"\"\n",
        "        self._check_param_consistency(transition_probs, rewards)\n",
        "        self._transition_probs = transition_probs\n",
        "        self._rewards = rewards\n",
        "        self._initial_state = initial_state\n",
        "        self.n_states = len(transition_probs)\n",
        "        self.reset()\n",
        "        self.np_random, _ = seeding.np_random(seed)\n",
        "\n",
        "    def get_all_states(self):\n",
        "        \"\"\" return a tuple of all possiblestates \"\"\"\n",
        "        return tuple(self._transition_probs.keys())\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        \"\"\" return a tuple of possible actions in a given state \"\"\"\n",
        "        return tuple(self._transition_probs.get(state, {}).keys())\n",
        "\n",
        "    def is_terminal(self, state):\n",
        "        \"\"\" return True if state is terminal or False if it isn't \"\"\"\n",
        "        return len(self.get_possible_actions(state)) == 0\n",
        "\n",
        "    def get_next_states(self, state, action):\n",
        "        \"\"\" return a dictionary of {next_state1 : P(next_state1 | state, action), next_state2: ...} \"\"\"\n",
        "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._transition_probs[state][action]\n",
        "\n",
        "    def get_transition_prob(self, state, action, next_state):\n",
        "        \"\"\" return P(next_state | state, action) \"\"\"\n",
        "        return self.get_next_states(state, action).get(next_state, 0.0)\n",
        "\n",
        "    def get_reward(self, state, action, next_state):\n",
        "        \"\"\" return the reward you get for taking action in state and landing on next_state\"\"\"\n",
        "        assert action in self.get_possible_actions(state), \"cannot do action %s from state %s\" % (action, state)\n",
        "        return self._rewards.get(state, {}).get(action, {}).get(next_state, 0.0)\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\" reset the game, return the initial state\"\"\"\n",
        "        if self._initial_state is None:\n",
        "            self._current_state = self.np_random.choice(\n",
        "                tuple(self._transition_probs.keys()))\n",
        "        elif self._initial_state in self._transition_probs:\n",
        "            self._current_state = self._initial_state\n",
        "        elif callable(self._initial_state):\n",
        "            self._current_state = self._initial_state()\n",
        "        else:\n",
        "            raise ValueError(\n",
        "                \"initial state %s should be either a state or a function() -> state\" % self._initial_state)\n",
        "        return self._current_state\n",
        "def get_action_value(mdp, state_values, state, action, gamma):\n",
        "    \"\"\" Computes Q(s,a) as in formula above \"\"\"\n",
        "    Q = [mdp.get_transition_prob(state, action, s) * (mdp.get_reward(state, action, s) + gamma * state_values[s]) for s in state_values]\n",
        "    return sum(Q)\n",
        "\n",
        "def get_new_state_value(mdp, state_values, state, gamma):\n",
        "    \"\"\" Computes next V(s) as in formula above. Please do not change state_values in process. \"\"\"\n",
        "    if mdp.is_terminal(state): return 0\n",
        "\n",
        "    Q = [get_action_value(mdp, state_values, state, a, gamma) for a in mdp.get_possible_actions(state)]\n",
        "    return max(Q)\n",
        "\n",
        "# parameters\n",
        "gamma = 0.9            # discount for MDP\n",
        "num_iter = 1000         # maximum iterations, excluding initialization\n",
        "min_difference = 0.001 # stop VI if new values are this close to old values (or closer)\n",
        "\n",
        "# initialize V(s)\n",
        "state_values = {s : 0 for s in mdp.get_all_states()}\n",
        "\n",
        "for i in range(num_iter):\n",
        "    \n",
        "    # Compute new state values using the functions you defined above.\n",
        "    # It must be a dict {state : float V_new(state)}\n",
        "    new_state_values = { state: get_new_state_value(mdp, state_values, state, gamma)\n",
        "        for state in mdp.get_all_states()\n",
        "    }\n",
        "    \n",
        "    print()\n",
        "\n",
        "    assert isinstance(new_state_values, dict)\n",
        "    \n",
        "    # Compute difference\n",
        "    diff = max(abs(new_state_values[s] - state_values[s]) for s in mdp.get_all_states())\n",
        "    print(\"iter %4i   |   \"%(i,), end=\"\")\n",
        "    print('   '.join(\"V(%s) = %.3f\"%(s, v) for s,v in state_values.items()), end='\\n\\n')\n",
        "    state_values = new_state_values\n",
        "    \n",
        "    if diff < min_difference:\n",
        "        print(\"Terminated\"); break\n",
        "        \n",
        "if has_graphviz:\n",
        "    display(plot_graph_with_state_values(mdp, state_values))\n",
        "print()\n",
        "print(\"Final state values:\", state_values)"
      ],
      "metadata": {
        "id": "lSuwm1rQUeit"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}